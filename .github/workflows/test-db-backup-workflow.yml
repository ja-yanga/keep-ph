name: supabase-db-backup-test

on:
  workflow_dispatch:

jobs:
  backup-test:
    runs-on: ubuntu-latest

    env:
      SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
      BACKUP_ENABLED: ${{ vars.BACKUP_ENABLED || 'true' }} # Repository variable (defaults to 'true' if not set)

    steps:
      - name: Check if backups are enabled
        run: |
          if [ "$BACKUP_ENABLED" != "true" ]; then
            echo "Backups are disabled. Exiting workflow."
            exit 0
          fi

      - name: Checkout repo
        uses: actions/checkout@v4
        # Note: This workflow can run from any branch, but the workflow file
        # must exist in that branch for it to appear in GitHub Actions UI.
        # If you don't see this workflow, merge it from master to your branch.

      - name: Setup Supabase CLI
        uses: supabase/setup-cli@v1
        with:
          version: latest

      - name: Verify database connection
        run: |
          echo "Testing database connection..."
          # Check if connection string is set
          if [ -z "$SUPABASE_DB_URL" ]; then
            echo "Error: SUPABASE_DB_URL is not set"
            exit 1
          fi

          # Verify connection string format
          if echo "$SUPABASE_DB_URL" | grep -q "pooler"; then
            echo "✓ Using connection pooler URL"
            if echo "$SUPABASE_DB_URL" | grep -q ":5432"; then
              echo "⚠ Warning: Using port 5432 (direct connection). For backups, use Session mode pooler (port 6543)"
            elif echo "$SUPABASE_DB_URL" | grep -q ":6543"; then
              echo "✓ Using pooler port 6543"
            fi
          else
            echo "⚠ Warning: Not using connection pooler URL"
            echo "For database backups, use the 'Session' mode connection string from Supabase dashboard"
            echo "Format: postgresql://postgres.[ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres"
            echo "Note: Session mode is recommended for long-running operations like backups"
          fi

          # Test basic connectivity
          if command -v psql > /dev/null 2>&1; then
            echo "Testing connection with psql..."
            # Extract connection details and test (without exposing password)
            echo "Connection test will be performed during dump"
          fi

      - name: Set timestamp and filename
        run: |
          TIMESTAMP=$(date +"%Y-%m-%d_%H-%M-%S")
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_ENV
          echo "FILENAME=supabase-backup-$TIMESTAMP.tar.gz" >> $GITHUB_ENV

      - name: Create backup folder
        run: mkdir -p backups

      - name: Dump roles
        run: |
          echo "Dumping database roles..."
          supabase db dump \
            --db-url "$SUPABASE_DB_URL" \
            --role-only \
            -f backups/roles.sql || {
              echo "Error: Failed to dump roles"
              exit 1
            }
          echo "✓ Roles dumped successfully"
          ls -lh backups/roles.sql

      - name: Dump schema
        run: |
          echo "Dumping database schema..."
          supabase db dump \
            --db-url "$SUPABASE_DB_URL" \
            -f backups/schema.sql || {
              echo "Error: Failed to dump schema"
              exit 1
            }
          echo "✓ Schema dumped successfully"
          ls -lh backups/schema.sql

      - name: Dump data
        run: |
          echo "Dumping database data..."
          supabase db dump \
            --db-url "$SUPABASE_DB_URL" \
            --data-only \
            --use-copy \
            -f backups/data.sql || {
              echo "Error: Failed to dump data"
              echo "This might happen if:"
              echo "1. Database connection failed"
              echo "2. Database has no data"
              echo "3. Connection pooler timeout (use Session mode)"
              exit 1
            }
          echo "✓ Data dumped successfully"
          ls -lh backups/data.sql
          echo ""
          echo "Data file size: $(du -h backups/data.sql | cut -f1)"
          echo "Data file line count: $(wc -l < backups/data.sql)"
          if [ ! -s backups/data.sql ]; then
            echo "⚠ Warning: data.sql file is empty or doesn't exist"
          fi

      # -------------------------------
      # Backup Storage Buckets
      # -------------------------------
      - name: Create storage backup folder
        run: mkdir -p backups/storage

      - name: Install jq for JSON parsing
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Backup storage buckets
        env:
          SUPABASE_URL: https://${{ secrets.SUPABASE_PROJECT_REF }}.supabase.co
          SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          BUCKETS_TO_BACKUP: ${{ vars.STORAGE_BUCKETS_TO_BACKUP || 'all' }}
        run: |
          # Function to download all files from a bucket
          backup_bucket() {
            local bucket_name=$1
            local bucket_dir="backups/storage/$bucket_name"
            mkdir -p "$bucket_dir"
            
            echo "Backing up bucket: $bucket_name"
            
            # List all files in the bucket (with pagination)
            local offset=0
            local limit=1000
            local total_files=0
            
            while true; do
              local response=$(curl -s -X POST \
                "$SUPABASE_URL/storage/v1/object/list/$bucket_name" \
                -H "Authorization: Bearer $SUPABASE_KEY" \
                -H "Content-Type: application/json" \
                -d "{\"limit\": $limit, \"offset\": $offset}")
              
              # Check if response is valid JSON array
              if ! echo "$response" | jq -e 'type == "array"' > /dev/null 2>&1; then
                break
              fi
              
              # Get file count in this batch
              local file_count=$(echo "$response" | jq 'length')
              if [ "$file_count" -eq 0 ]; then
                break
              fi
              
              # Extract and download files
              local files=$(echo "$response" | jq -r '.[]?.name // empty')
              while IFS= read -r file_name; do
                if [ -n "$file_name" ]; then
                  total_files=$((total_files + 1))
                  
                  # Create directory structure for nested files
                  local file_dir=$(dirname "$file_name")
                  if [ "$file_dir" != "." ]; then
                    mkdir -p "$bucket_dir/$file_dir"
                  fi
                  
                  # Download the file
                  echo "  Downloading: $file_name"
                  if curl -s -f -X GET \
                    "$SUPABASE_URL/storage/v1/object/$bucket_name/$file_name" \
                    -H "Authorization: Bearer $SUPABASE_KEY" \
                    -o "$bucket_dir/$file_name"; then
                    echo "    ✓ Downloaded: $file_name"
                  else
                    echo "    ✗ Failed to download: $file_name"
                  fi
                fi
              done <<< "$files"
              
              # If we got fewer files than the limit, we're done
              if [ "$file_count" -lt "$limit" ]; then
                break
              fi
              
              offset=$((offset + limit))
            done
            
            echo "Completed backup of bucket: $bucket_name (total files: $total_files)"
          }

          # Get list of buckets to backup
          if [ "$BUCKETS_TO_BACKUP" = "all" ]; then
            echo "Backing up all buckets..."
            # List all buckets
            buckets_response=$(curl -s -X GET \
              "$SUPABASE_URL/storage/v1/bucket" \
              -H "Authorization: Bearer $SUPABASE_KEY")
            
            # Extract bucket names using jq
            echo "$buckets_response" | jq -r '.[]?.name // empty' | while IFS= read -r bucket_name; do
              if [ -n "$bucket_name" ] && [ "$bucket_name" != "DB-BACKUPS" ]; then
                # Skip the DB-BACKUPS bucket to avoid backing up backups
                backup_bucket "$bucket_name"
              fi
            done
          else
            # Backup specific buckets (comma-separated)
            echo "$BUCKETS_TO_BACKUP" | tr ',' '\n' | while IFS= read -r bucket_name; do
              if [ -n "$bucket_name" ]; then
                backup_bucket "$bucket_name"
              fi
            done
          fi

          # Create a manifest of backed up buckets
          if [ -d "backups/storage" ] && [ "$(ls -A backups/storage 2>/dev/null)" ]; then
            echo "Storage backup completed. Summary:"
            du -sh backups/storage/*/ 2>/dev/null | sort -h || true
            echo "Buckets backed up:"
            ls -d backups/storage/*/ 2>/dev/null | sed 's|backups/storage/||' | sed 's|/$||' || echo "No buckets found"
          else
            echo "No storage buckets to backup or all buckets are empty"
            touch backups/storage/.empty
          fi

      - name: Compress backup
        run: |
          tar -czf ${{ env.FILENAME }} backups

      - name: Verify backup files
        run: |
          echo "=== Backup Verification ==="
          echo ""
          echo "Backup created: ${{ env.FILENAME }}"
          ls -lh ${{ env.FILENAME }}
          echo ""
          echo "Backup size: $(du -h ${{ env.FILENAME }} | cut -f1)"
          echo ""
          echo "=== Individual File Sizes (before compression) ==="
          echo "Roles: $(du -h backups/roles.sql 2>/dev/null | cut -f1 || echo 'NOT FOUND')"
          echo "Schema: $(du -h backups/schema.sql 2>/dev/null | cut -f1 || echo 'NOT FOUND')"
          echo "Data: $(du -h backups/data.sql 2>/dev/null | cut -f1 || echo 'NOT FOUND')"
          echo ""
          echo "=== Data File Details ==="
          if [ -f backups/data.sql ]; then
            echo "Data file exists: ✓"
            echo "Data file size: $(du -h backups/data.sql | cut -f1)"
            echo "Data file lines: $(wc -l < backups/data.sql)"
            echo "First 5 lines of data.sql:"
            head -5 backups/data.sql || echo "  (file is empty or unreadable)"
          else
            echo "Data file exists: ✗ NOT FOUND"
          fi
          echo ""
          echo "=== Backup Contents (first 30 entries) ==="
          tar -tzf ${{ env.FILENAME }} | head -30
          echo ""
          echo "=== Backup Structure ==="
          echo "- Database files:"
          tar -tzf ${{ env.FILENAME }} | grep -E '^(backups/)?(roles|schema|data)\.sql$' || echo "  (none found)"
          echo "- Storage buckets:"
          tar -tzf ${{ env.FILENAME }} | grep '^backups/storage/' | cut -d'/' -f3 | sort -u | head -10 || echo "  (none found)"
          echo ""
          echo "Total files in backup: $(tar -tzf ${{ env.FILENAME }} | wc -l)"

      # -------------------------------
      # Upload to Supabase Storage
      # -------------------------------
      - name: Upload to Supabase Storage
        run: |
          echo "Uploading backup to Supabase Storage..."
          echo "File: ${{ env.FILENAME }}"
          echo "Destination: DB-BACKUPS/${{ env.FILENAME }}"

          response=$(curl -s -w "\n%{http_code}" -X POST \
            "https://${{ secrets.SUPABASE_PROJECT_REF }}.supabase.co/storage/v1/object/DB-BACKUPS/${{ env.FILENAME }}" \
            -H "Authorization: Bearer ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" \
            -H "Content-Type: application/gzip" \
            --data-binary "@${{ env.FILENAME }}")

          http_code=$(echo "$response" | tail -n1)
          body=$(echo "$response" | sed '$d')

          if [ "$http_code" -eq 200 ] || [ "$http_code" -eq 201 ]; then
            echo "✓ Successfully uploaded to Supabase Storage (HTTP $http_code)"
            echo "Backup URL: https://${{ secrets.SUPABASE_PROJECT_REF }}.supabase.co/storage/v1/object/public/DB-BACKUPS/${{ env.FILENAME }}"
          else
            echo "✗ Failed to upload to Supabase Storage (HTTP $http_code)"
            echo "Response: $body"
            exit 1
          fi

      # -------------------------------
      # Upload to Cloudflare R2
      # -------------------------------
      - name: Configure AWS CLI for R2
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.R2_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Upload to Cloudflare R2
        env:
          # R2 endpoint format: https://{account_id}.r2.cloudflarestorage.com
          # Account ID should be set in R2_ACCOUNT_ID secret (e.g., 2cb950ce770f7b7ed02396ece33cfe75)
          R2_ENDPOINT: https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
        run: |
          echo "Uploading backup to Cloudflare R2..."
          echo "File: ${{ env.FILENAME }}"
          echo "Endpoint: $R2_ENDPOINT"
          echo "Destination: s3://${{ secrets.R2_BUCKET }}/${{ env.FILENAME }}"

          if aws s3 cp \
            ${{ env.FILENAME }} \
            s3://${{ secrets.R2_BUCKET }}/${{ env.FILENAME }} \
            --endpoint-url $R2_ENDPOINT; then
            echo "✓ Successfully uploaded to Cloudflare R2"
            echo "Backup location: s3://${{ secrets.R2_BUCKET }}/${{ env.FILENAME }}"
          else
            echo "✗ Failed to upload to Cloudflare R2"
            exit 1
          fi

      - name: Backup completion summary
        run: |
          echo "=== Backup Test Completed Successfully ==="
          echo ""
          echo "Backup file: ${{ env.FILENAME }}"
          echo "Backup size: $(du -h ${{ env.FILENAME }} | cut -f1)"
          echo ""
          echo "Uploaded to:"
          echo "  ✓ Supabase Storage: DB-BACKUPS/${{ env.FILENAME }}"
          echo "  ✓ Cloudflare R2: s3://${{ secrets.R2_BUCKET }}/${{ env.FILENAME }}"
          echo ""
          echo "You can verify the backup by:"
          echo "  1. Checking Supabase Storage dashboard for the 'DB-BACKUPS' bucket"
          echo "  2. Checking Cloudflare R2 dashboard for your bucket"
